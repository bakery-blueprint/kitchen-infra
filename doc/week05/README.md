# 인프라를 지탱하는 으용 이론

## 1. 캐시

Cache란 나중에 요청할 결과를 미리 저장해둔 후 빠르게 서비스 해주는 것을 의미한다.

매커니즘은 파레토 법칙이 있다. 

파레토 법칙 : 80% 결과는 20%의 원인으로 발생한다. 즉 많이 발생하는 20% 캐싱하면 극대화한다.



### 로컬 vs 글로벌

- Local Cache
  - 서버마다 캐시를 따로 저장한다.
  - 다른 서버의 캐시를 참조하기 어렵다.
  - 서버 내에서 작동하기 때문에 속도가 빠르다.
  - 로컬 서버 장비의 Resource를 이용한다. (Memory, Disk)
  - 캐시에 저장된 데이터가 변경되는 경우:
    - 해당 서버를 제외한 모든 peer에 변경 사항 전달
    - All-to-All Replication
    - WAS 인스턴스가 늘어나고, 캐시 저장 데이터 크기가 커지면 성능이 저하되는 이유는 이 때문
- Global Cache
  - 여러 서버에서 캐시 서버에 접근하여 참조 할 수 있다.
  - 별도의 캐시 서버를 이용하기 때문에 서버 간 데이터 공유가 쉽다.
  - 네트워크 트래픽을 사용해야 해서 로컬 캐시보다는 느리다.
  - 데이터를 분산하여 저장 할 수 있다.
    - Replication: 두 개의 이상의 DBMS 시스템을 Mater / Slave 로 나눠서 동일한 데이터를 저장하는 방식
    - Sharding: 같은 테이블 스키마를 가진 데이터를 다수의 데이터베이스에 분산하여 저장하는 방법
  - 캐시에 저장된 데이터가 변경되는 경우:
    - 추가적인 작업 필요없음
    - 서비스 확장으로 WAS 인스턴스가 늘어나고, Cache 데이터 크기가 커질 수록 효과적인 이유



### 강대명 캐시의 모든것

- 애플리케이션 단에 메모리(로컬 캐시)랑 별도의 인메모리 DB쓰는 거랑 차이점?

  A : 로컬 캐시가 가장 빠르지만, 동기화가 어렵다. 

- 트랜잭션과 캐시

  A : 트랜잭션 개념을 구현하기 힘들다. 

- 캐시가 죽었을때 가정을 해봤을까..??

  A: 캐시가 죽으면 DB까지 죽을 경우를 대비해야한다. 	

https://youtu.be/zkbvFOwJFgA

